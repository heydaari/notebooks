{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport jax\nimport numpy as np\nimport tensorflow as tf\nimport keras\n\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import Mesh\nfrom jax.sharding import NamedSharding\nfrom jax.sharding import PartitionSpec as P","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:21:15.786002Z","iopub.execute_input":"2025-01-05T18:21:15.786279Z","iopub.status.idle":"2025-01-05T18:21:22.732189Z","shell.execute_reply.started":"2025-01-05T18:21:15.786247Z","shell.execute_reply":"2025-01-05T18:21:22.731492Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(jax.devices())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:22:04.343278Z","iopub.execute_input":"2025-01-05T18:22:04.343840Z","iopub.status.idle":"2025-01-05T18:22:05.105350Z","shell.execute_reply.started":"2025-01-05T18:22:04.343811Z","shell.execute_reply":"2025-01-05T18:22:05.104627Z"}},"outputs":[{"name":"stdout","text":"[cuda(id=0), cuda(id=1)]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def get_model():\n    # Make a simple convnet with batch normalization and dropout.\n    inputs = keras.Input(shape=(28, 28, 1))\n    x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n        x\n    )\n    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.Conv2D(\n        filters=24,\n        kernel_size=6,\n        use_bias=False,\n        strides=2,\n    )(x)\n    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.Conv2D(\n        filters=32,\n        kernel_size=6,\n        padding=\"same\",\n        strides=2,\n        name=\"large_k\",\n    )(x)\n    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    outputs = keras.layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    return model\n\n\ndef get_datasets():\n    # Load the data and split it between train and test sets\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    # Scale images to the [0, 1] range\n    x_train = x_train.astype(\"float32\")\n    x_test = x_test.astype(\"float32\")\n    # Make sure images have shape (28, 28, 1)\n    x_train = np.expand_dims(x_train, -1)\n    x_test = np.expand_dims(x_test, -1)\n    print(\"x_train shape:\", x_train.shape)\n    print(x_train.shape[0], \"train samples\")\n    print(x_test.shape[0], \"test samples\")\n\n    # Create TF Datasets\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    return train_data, eval_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:22:21.629747Z","iopub.execute_input":"2025-01-05T18:22:21.630058Z","iopub.status.idle":"2025-01-05T18:22:21.638276Z","shell.execute_reply.started":"2025-01-05T18:22:21.630029Z","shell.execute_reply":"2025-01-05T18:22:21.637428Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Config\nnum_epochs = 2\nbatch_size = 64\n\ntrain_data, eval_data = get_datasets()\ntrain_data = train_data.batch(batch_size, drop_remainder=True)\n\nmodel = get_model()\noptimizer = keras.optimizers.Adam(1e-3)\nloss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Initialize all state with .build()\n(one_batch, one_batch_labels) = next(iter(train_data))\nmodel.build(one_batch)\noptimizer.build(model.trainable_variables)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:22:32.001935Z","iopub.execute_input":"2025-01-05T18:22:32.002211Z","iopub.status.idle":"2025-01-05T18:22:34.865545Z","shell.execute_reply.started":"2025-01-05T18:22:32.002188Z","shell.execute_reply":"2025-01-05T18:22:34.864818Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# This is the loss function that will be differentiated.\n# Keras provides a pure functional forward pass: model.stateless_call\ndef compute_loss(trainable_variables, non_trainable_variables, x, y):\n    y_pred, updated_non_trainable_variables = model.stateless_call(\n        trainable_variables, non_trainable_variables, x, training=True\n    )\n    loss_value = loss(y, y_pred)\n    return loss_value, updated_non_trainable_variables\n\n\n# Function to compute gradients\ncompute_gradients = jax.value_and_grad(compute_loss, has_aux=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:23:55.555965Z","iopub.execute_input":"2025-01-05T18:23:55.556291Z","iopub.status.idle":"2025-01-05T18:23:55.560962Z","shell.execute_reply.started":"2025-01-05T18:23:55.556261Z","shell.execute_reply":"2025-01-05T18:23:55.559977Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training step, Keras provides a pure functional optimizer.stateless_apply\n@jax.jit\ndef train_step(train_state, x, y):\n    trainable_variables, non_trainable_variables, optimizer_variables = train_state\n    (loss_value, non_trainable_variables), grads = compute_gradients(\n        trainable_variables, non_trainable_variables, x, y\n    )\n\n    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n        optimizer_variables, grads, trainable_variables\n    )\n\n    return loss_value, (\n        trainable_variables,\n        non_trainable_variables,\n        optimizer_variables,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:24:06.921274Z","iopub.execute_input":"2025-01-05T18:24:06.921589Z","iopub.status.idle":"2025-01-05T18:24:06.925987Z","shell.execute_reply.started":"2025-01-05T18:24:06.921565Z","shell.execute_reply":"2025-01-05T18:24:06.925197Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Replicate the model and optimizer variable on all devices\ndef get_replicated_train_state(devices):\n    # All variables will be replicated on all devices\n    var_mesh = Mesh(devices, axis_names=(\"_\"))\n    # In NamedSharding, axes not mentioned are replicated (all axes here)\n    var_replication = NamedSharding(var_mesh, P())\n\n    # Apply the distribution settings to the model variables\n    trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n    non_trainable_variables = jax.device_put(\n        model.non_trainable_variables, var_replication\n    )\n    optimizer_variables = jax.device_put(optimizer.variables, var_replication)\n\n    # Combine all state in a tuple\n    return (trainable_variables, non_trainable_variables, optimizer_variables)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:24:25.226805Z","iopub.execute_input":"2025-01-05T18:24:25.227076Z","iopub.status.idle":"2025-01-05T18:24:25.231548Z","shell.execute_reply.started":"2025-01-05T18:24:25.227055Z","shell.execute_reply":"2025-01-05T18:24:25.230705Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_devices = len(jax.local_devices())\nprint(f\"Running on {num_devices} devices: {jax.local_devices()}\")\ndevices = mesh_utils.create_device_mesh((num_devices,))\n\n# Data will be split along the batch axis\ndata_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\ndata_sharding = NamedSharding(\n    data_mesh,\n    P(\n        \"batch\",\n    ),\n)  # naming axes of the sharded partition","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:24:40.414150Z","iopub.execute_input":"2025-01-05T18:24:40.414500Z","iopub.status.idle":"2025-01-05T18:24:40.420177Z","shell.execute_reply.started":"2025-01-05T18:24:40.414469Z","shell.execute_reply":"2025-01-05T18:24:40.419407Z"}},"outputs":[{"name":"stdout","text":"Running on 2 devices: [cuda(id=0), cuda(id=1)]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Display data sharding\nx, y = next(iter(train_data))\nsharded_x = jax.device_put(x.numpy(), data_sharding)\nprint(\"Data sharding\")\njax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [-1, 28 * 28]))\n\ntrain_state = get_replicated_train_state(devices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:24:52.052098Z","iopub.execute_input":"2025-01-05T18:24:52.052374Z","iopub.status.idle":"2025-01-05T18:24:52.343509Z","shell.execute_reply.started":"2025-01-05T18:24:52.052354Z","shell.execute_reply":"2025-01-05T18:24:52.342778Z"}},"outputs":[{"name":"stdout","text":"Data sharding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                     \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mGPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                                      \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                     \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mGPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                                      \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                     GPU 0                                      </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                     GPU 1                                      </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Custom training loop\nfor epoch in range(100):\n    data_iter = iter(train_data)\n    for data in data_iter:\n        x, y = data\n        sharded_x = jax.device_put(x.numpy(), data_sharding)\n        loss_value, train_state = train_step(train_state, sharded_x, y.numpy())\n    print(\"Epoch\", epoch, \"loss:\", loss_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T18:28:25.740895Z","iopub.execute_input":"2025-01-05T18:28:25.741289Z","iopub.status.idle":"2025-01-05T18:31:47.670954Z","shell.execute_reply.started":"2025-01-05T18:28:25.741255Z","shell.execute_reply":"2025-01-05T18:31:47.670201Z"}},"outputs":[{"name":"stdout","text":"Epoch 0 loss: 0.015642429\nEpoch 1 loss: 8.512389e-05\nEpoch 2 loss: 0.00097128894\nEpoch 3 loss: 0.00017838652\nEpoch 4 loss: 0.0016941413\nEpoch 5 loss: 0.0033031378\nEpoch 6 loss: 0.0008292221\nEpoch 7 loss: 0.0012198741\nEpoch 8 loss: 0.00859246\nEpoch 9 loss: 0.00043380394\nEpoch 10 loss: 0.0349427\nEpoch 11 loss: 8.1968516e-05\nEpoch 12 loss: 0.001320726\nEpoch 13 loss: 6.179434e-05\nEpoch 14 loss: 0.00024736577\nEpoch 15 loss: 0.0011571057\nEpoch 16 loss: 0.003857566\nEpoch 17 loss: 0.07024604\nEpoch 18 loss: 2.021938e-05\nEpoch 19 loss: 0.00043419388\nEpoch 20 loss: 2.0674895e-06\nEpoch 21 loss: 0.0019072035\nEpoch 22 loss: 4.773264e-05\nEpoch 23 loss: 9.284496e-05\nEpoch 24 loss: 0.010036009\nEpoch 25 loss: 0.009936109\nEpoch 26 loss: 3.162212e-05\nEpoch 27 loss: 0.0011522755\nEpoch 28 loss: 0.00034596364\nEpoch 29 loss: 0.006314157\nEpoch 30 loss: 0.00015106531\nEpoch 31 loss: 2.9397821e-05\nEpoch 32 loss: 0.0006282042\nEpoch 33 loss: 0.0003507756\nEpoch 34 loss: 0.00049265556\nEpoch 35 loss: 0.008983544\nEpoch 36 loss: 1.00759e-05\nEpoch 37 loss: 0.000566489\nEpoch 38 loss: 5.885661e-05\nEpoch 39 loss: 0.015190241\nEpoch 40 loss: 3.719585e-05\nEpoch 41 loss: 0.0021061338\nEpoch 42 loss: 2.9847844e-05\nEpoch 43 loss: 0.06472202\nEpoch 44 loss: 7.8231054e-08\nEpoch 45 loss: 7.400239e-05\nEpoch 46 loss: 3.7811543e-07\nEpoch 47 loss: 1.6074114e-06\nEpoch 48 loss: 4.627045e-05\nEpoch 49 loss: 0.042778216\nEpoch 50 loss: 0.0022369681\nEpoch 51 loss: 0.00018533618\nEpoch 52 loss: 1.8309457e-06\nEpoch 53 loss: 1.4137245e-06\nEpoch 54 loss: 0.00013319541\nEpoch 55 loss: 0.00016588192\nEpoch 56 loss: 0.0020393392\nEpoch 57 loss: 1.965424e-05\nEpoch 58 loss: 0.0030651311\nEpoch 59 loss: 1.8365118e-06\nEpoch 60 loss: 0.0007345971\nEpoch 61 loss: 0.029760048\nEpoch 62 loss: 8.868195e-06\nEpoch 63 loss: 0.009009222\nEpoch 64 loss: 1.4901151e-07\nEpoch 65 loss: 0.000966682\nEpoch 66 loss: 7.996258e-06\nEpoch 67 loss: 2.812583e-07\nEpoch 68 loss: 7.760068e-06\nEpoch 69 loss: 6.422015e-05\nEpoch 70 loss: 2.1074484e-05\nEpoch 71 loss: 0.0034191725\nEpoch 72 loss: 0.0040749456\nEpoch 73 loss: 2.9883651e-05\nEpoch 74 loss: 0.00014483585\nEpoch 75 loss: 0.00022326347\nEpoch 76 loss: 9.884788e-05\nEpoch 77 loss: 0.014305741\nEpoch 78 loss: 7.171137e-07\nEpoch 79 loss: 4.0232885e-07\nEpoch 80 loss: 1.415608e-07\nEpoch 81 loss: 1.0153741e-05\nEpoch 82 loss: 5.8444016e-06\nEpoch 83 loss: 1.7097373e-05\nEpoch 84 loss: 3.0360977e-07\nEpoch 85 loss: 1.2675884e-05\nEpoch 86 loss: 8.971862e-06\nEpoch 87 loss: 2.1717772e-06\nEpoch 88 loss: 5.178118e-07\nEpoch 89 loss: 4.0573508e-05\nEpoch 90 loss: 0.00013616931\nEpoch 91 loss: 6.377846e-05\nEpoch 92 loss: 0.00015198317\nEpoch 93 loss: 8.9576e-06\nEpoch 94 loss: 2.2249218e-05\nEpoch 95 loss: 1.51222275e-05\nEpoch 96 loss: 2.9103681e-05\nEpoch 97 loss: 1.7172983e-06\nEpoch 98 loss: 0.0001549618\nEpoch 99 loss: 1.3303661e-05\n","output_type":"stream"}],"execution_count":14}]}