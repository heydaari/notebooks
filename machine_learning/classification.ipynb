{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification, KNN, and Logistic Regression\n",
        "\n",
        "## Introduction to Classification\n",
        "\n",
        "\n",
        "\n",
        "Classification is a fundamental task in **Supervised Machine Learning**. The goal of classification is to train a model using a dataset of labeled examples (where we know the correct category for each example) so that the model can accurately predict the category (or \"class\") for new, unseen data points.\n",
        "\n",
        "Think of it like sorting objects into predefined bins. You learn the rules by looking at examples that are already sorted, and then you use those rules to sort new objects.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "*   **Supervised:** Requires labeled training data (input features and their corresponding correct class labels).\n",
        "*   **Discrete Output:** The predicted output is a category or class label, not a continuous number (like predicting house prices, which is Regression).\n",
        "*   **Goal:** To learn a decision boundary that separates different classes in the feature space.\n",
        "\n",
        "**Types of Classification:**\n",
        "\n",
        "1.  **Binary Classification:** Problems where there are only two possible outcomes or classes.\n",
        "    *   *Example:* Spam detection (email is either \"spam\" or \"not spam\").\n",
        "    *   *Example:* Medical test result (patient has the \"disease\" or \"does not have the disease\").\n",
        "2.  **Multiclass Classification:** Problems where there are more than two possible outcomes or classes.\n",
        "    *   *Example:* Image classification (image could be a \"cat\", \"dog\", \"car\", or \"person\").\n",
        "    *   *Example:* Sentiment analysis (text could be \"positive\", \"negative\", or \"neutral\").\n",
        "\n",
        "**How is Classification Applied in the Real World?**\n",
        "\n",
        "Classification algorithms are used across various domains:\n",
        "\n",
        "*   **Email Filtering:** Classifying emails as spam or not spam.\n",
        "*   **Medical Diagnosis:** Predicting whether a patient has a certain disease based on symptoms or medical imaging (e.g., classifying tumors as malignant or benign).\n",
        "*   **Image Recognition:** Identifying objects within images (e.g., autonomous driving systems identifying pedestrians, traffic lights, other vehicles).\n",
        "*   **Sentiment Analysis:** Determining the sentiment expressed in a piece of text (e.g., positive, negative, neutral review).\n",
        "*   **Document Classification:** Assigning topics or categories to documents (e.g., news articles classified as \"sports\", \"politics\", \"technology\").\n",
        "\n",
        "---\n",
        "\n",
        "##  K-Nearest Neighbors (KNN)\n",
        "\n",
        "![Sample Image](https://miro.medium.com/v2/resize:fit:505/0*2_qzcm2gSe9l67aI.png)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is one of the simplest and most intuitive classification algorithms. The core idea is based on the principle: **\"You are similar to your neighbors.\"**\n",
        "\n",
        "Imagine you have a map with different houses plotted on it, each colored based on who lives there (e.g., blue for Democrats, red for Republicans). Now, a new house is built. To predict the political leaning of the new resident, KNN would look at the 'K' closest existing houses (neighbors) to the new one. If the majority of the K closest neighbors are, say, blue, KNN would predict that the new resident is also likely to be a Democrat.\n",
        "\n",
        "**How KNN Works :**\n",
        "\n",
        "1.  **Choose 'K':** Decide how many neighbors (K) to consider. This is a crucial parameter.\n",
        "2.  **Calculate Distances:** When a new, unclassified data point arrives, calculate the distance between this new point and *every* point in the training dataset. Common distance measures include Euclidean distance (the straight-line distance).\n",
        "3.  **Identify Neighbors:** Find the 'K' training data points that are closest (have the smallest distances) to the new point. These are its \"K nearest neighbors\".\n",
        "4.  **Majority Vote:** Look at the class labels of these K neighbors. The new data point is assigned the class label that is most common among its K nearest neighbors. (In case of a tie, strategies like picking randomly, or preferring the absolute closest neighbor might be used).\n",
        "\n",
        "**Key notes:**\n",
        "\n",
        "*   **Lazy Learner:** KNN is called a \"lazy learner\" because it doesn't build an explicit model during the training phase. It simply stores the entire training dataset. The real computation happens during the prediction phase.\n",
        "*   **Instance-Based:** It makes predictions based on the similarity to specific instances in the training data.\n",
        "*   **Importance of K:** Choosing the right value for K is critical. A small K might be sensitive to noise, while a large K might smooth out the decision boundary too much.\n",
        "*   **Feature Scaling:** KNN is sensitive to the scale of features (e.g., if one feature ranges from 0-1 and another from 0-1000, the second feature will dominate the distance calculation). It's usually essential to scale features (e.g., to a 0-1 range or standardize them) before applying KNN.\n",
        "\n",
        "**Simple Python Implementation (scikit-learn):**"
      ],
      "metadata": {
        "id": "ib1Kdz3i6u_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3y3n47e6n44",
        "outputId": "46261f3e-dc5a-40cd-8c3f-d982c15d2bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "Dataset Description (Partial):\n",
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 569\n",
            "\n",
            ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            ":Attribute Information:\n",
            "    - radius (mean of distances from center to points on the perimeter)\n",
            "    - texture (standard deviation of gray-scale values)\n",
            "    - perimeter\n",
            "    - area\n",
            "    - smoothness (local variation in radius lengths)\n",
            "    - compactness (...\n",
            "\n",
            "Feature Names:\n",
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "\n",
            "Target Names:\n",
            "['malignant' 'benign']\n",
            "\n",
            "Shape of Features (X): (569, 30)\n",
            "Shape of Target (y): (569,)\n",
            "------------------------------\n",
            "\n",
            "--- Splitting Data ---\n",
            "Shape of X_train: (398, 30)\n",
            "Shape of y_train: (398,)\n",
            "Shape of X_test: (171, 30)\n",
            "Shape of y_test: (171,)\n",
            "------------------------------\n",
            "\n",
            "--- Scaling Features ---\n",
            "Features scaled using StandardScaler.\n",
            "------------------------------\n",
            "\n",
            "--- Training the KNN Model ---\n",
            "Using KNeighborsClassifier with K=5\n",
            "Model training complete.\n",
            "------------------------------\n",
            "\n",
            "--- Making Predictions on the Test Set ---\n",
            "Predictions made on the scaled test set.\n",
            "------------------------------\n",
            "\n",
            "--- Evaluating the Model ---\n",
            "KNN (K=5) Model Accuracy: 0.9591\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 57   7]\n",
            " [  0 107]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.89      0.94        64\n",
            "      benign       0.94      1.00      0.97       107\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n",
            "------------------------------\n",
            "Evaluation Complete.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler # To scale features\n",
        "\n",
        "# 1. Load Data (Breast Cancer dataset)\n",
        "# Breast Cancer dataset has 2 classes (malignant, benign) based on 30 features\n",
        "print(\"--- Loading Data ---\")\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data  # Features\n",
        "y = cancer.target # Labels (0: malignant, 1: benign)\n",
        "\n",
        "# Print some basic information about the dataset\n",
        "print(\"Dataset Description (Partial):\")\n",
        "print(cancer.DESCR[:500] + \"...\") # Print the first 500 chars of the description\n",
        "print(\"\\nFeature Names:\")\n",
        "print(cancer.feature_names)\n",
        "print(\"\\nTarget Names:\")\n",
        "print(cancer.target_names)\n",
        "print(f\"\\nShape of Features (X): {X.shape}\")\n",
        "print(f\"Shape of Target (y): {y.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 2. Split Data into Training and Testing sets\n",
        "print(\"\\n--- Splitting Data ---\")\n",
        "# test_size=0.3 means 30% of data is for testing, 70% for training\n",
        "# random_state ensures reproducibility\n",
        "# stratify=y ensures the proportion of classes is the same in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 3. Feature Scaling (Important for KNN!)\n",
        "print(\"\\n--- Scaling Features ---\")\n",
        "# Scale data so that features have zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) # Use the same scaler fitted on training data\n",
        "print(\"Features scaled using StandardScaler.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 4. Create and Train the KNN Model\n",
        "print(\"\\n--- Training the KNN Model ---\")\n",
        "# Choose a value for K (n_neighbors)\n",
        "k = 5\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "print(f\"Using KNeighborsClassifier with K={k}\")\n",
        "\n",
        "# Train the model (for KNN, this mostly involves storing the training data)\n",
        "knn_classifier.fit(X_train_scaled, y_train)\n",
        "print(\"Model training complete.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 5. Make Predictions on the Test Set\n",
        "print(\"\\n--- Making Predictions on the Test Set ---\")\n",
        "y_pred = knn_classifier.predict(X_test_scaled)\n",
        "print(\"Predictions made on the scaled test set.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 6. Evaluate the Model\n",
        "print(\"\\n--- Evaluating the Model ---\")\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"KNN (K={k}) Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "# Rows: Actual Classes (0: Malignant, 1: Benign)\n",
        "# Columns: Predicted Classes (0: Malignant, 1: Benign)\n",
        "# [[TN, FP],\n",
        "#  [FN, TP]]\n",
        "# TN = True Negatives (Actual Malignant, Predicted Malignant)\n",
        "# FP = False Positives (Actual Malignant, Predicted Benign)\n",
        "# FN = False Negatives (Actual Benign, Predicted Malignant)\n",
        "# TP = True Positives (Actual Benign, Predicted Benign)\n",
        "\n",
        "\n",
        "# Generate Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "report = classification_report(y_test, y_pred, target_names=cancer.target_names)\n",
        "print(report)\n",
        "# Precision: TP / (TP + FP) - Of those predicted positive, how many were actually positive?\n",
        "# Recall (Sensitivity): TP / (TP + FN) - Of all actual positives, how many were correctly predicted?\n",
        "# F1-Score: Harmonic mean of Precision and Recall\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Evaluation Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Logistic Regression\n",
        "\n",
        "<img src=\"https://sdmntpritalynorth.oaiusercontent.com/files/00000000-c134-6246-bbd8-c6284b7dfe2e/raw?se=2025-04-16T15%3A23%3A33Z&sp=r&sv=2024-08-04&sr=b&scid=e4f002c1-460c-5c33-aad7-9230842e49da&skoid=59d06260-d7df-416c-92f4-051f0b47c607&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-16T06%3A17%3A26Z&ske=2025-04-17T06%3A17%3A26Z&sks=b&skv=2024-08-04&sig=H5qzV31oBSno2ktbiFkr1QXH0VSvbszmp96vfTC3WN4%3D\" alt=\"Example Logo\" width=\"50%\">\n",
        "\n",
        "Despite its name containing \"Regression\", Logistic Regression is a widely used algorithm for **Classification** problems, especially binary classification.\n",
        "\n",
        "Instead of directly predicting a class label, Logistic Regression models the **probability** that a given input point belongs to a particular class.\n",
        "\n",
        "Think of it as trying to draw a line (or a curve in higher dimensions) that best separates the different classes. For a new data point, the algorithm calculates its position relative to this line and uses that to estimate the probability of it belonging to the class on one side versus the other.\n",
        "\n",
        "**How Logistic Regression Works (Abstract Steps):**\n",
        "\n",
        "1.  **Linear Combination:** Just like Linear Regression, it starts by calculating a weighted sum of the input features, plus a bias term. `z = w1*x1 + w2*x2 + ... + wn*xn + b`\n",
        "2.  **Sigmoid Function:** The result `z` can be any real number. To convert this into a probability (which must be between 0 and 1), the result `z` is passed through a special mathematical function called the **Sigmoid function** (or Logistic function).\n",
        "    *   The Sigmoid function takes any real number and squashes it into the range (0, 1).\n",
        "    *   Large positive values of `z` result in a probability close to 1.\n",
        "    *   Large negative values of `z` result in a probability close to 0.\n",
        "    *   A value of `z = 0` results in a probability of 0.5.\n",
        "3.  **Probability Output:** The output of the Sigmoid function is interpreted as the probability of the data point belonging to the \"positive\" class (usually denoted as class 1). For example, P(Class=1). The probability of belonging to the \"negative\" class (Class=0) is simply 1 - P(Class=1).\n",
        "4.  **Decision Threshold:** A threshold (typically 0.5) is used to make the final classification decision.\n",
        "    *   If the predicted probability P(Class=1) is greater than the threshold (e.g., > 0.5), the model predicts Class 1.\n",
        "    *   If the predicted probability is less than or equal to the threshold (e.g., <= 0.5), the model predicts Class 0.\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "*   **Probability Estimates:** Provides probabilities, which can be useful for understanding confidence or for ranking predictions.\n",
        "*   **Linear Decision Boundary:** In its basic form, Logistic Regression finds a linear decision boundary. It works well when the classes are reasonably separable by a line or hyperplane. (It can be extended for non-linear boundaries using techniques like polynomial features or kernels, but the basic concept is linear).\n",
        "*   **Interpretability:** The weights (coefficients) learned by the model can often be interpreted to understand the influence of each feature on the prediction outcome (specifically, on the log-odds of the outcome).\n",
        "*   **Training:** Unlike KNN's lazy approach, Logistic Regression involves an active training process where the algorithm iteratively adjusts the weights (`w`) and bias (`b`) to minimize prediction errors on the training data (often using techniques like Gradient Descent).\n",
        "\n",
        "**Simple Python Implementation (scikit-learn):**"
      ],
      "metadata": {
        "id": "6ecv002u62ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load Data (Breast Cancer dataset)\n",
        "print(\"--- Loading Data ---\")\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data  # Features\n",
        "y = cancer.target # Labels (0: malignant, 1: benign)\n",
        "\n",
        "# Print some basic information about the dataset\n",
        "print(\"Dataset Description (Partial):\")\n",
        "print(cancer.DESCR[:500] + \"...\") # Print the first 500 chars of the description\n",
        "print(\"\\nFeature Names:\")\n",
        "print(cancer.feature_names)\n",
        "print(\"\\nTarget Names:\")\n",
        "print(cancer.target_names)\n",
        "print(f\"\\nShape of Features (X): {X.shape}\")\n",
        "print(f\"Shape of Target (y): {y.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 2. Split Data into Training and Testing sets\n",
        "print(\"\\n--- Splitting Data ---\")\n",
        "# test_size=0.3 means 30% of data is for testing, 70% for training\n",
        "# random_state ensures reproducibility\n",
        "# stratify=y ensures the proportion of classes is the same in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 3. Feature Scaling (Recommended for Logistic Regression)\n",
        "print(\"\\n--- Scaling Features ---\")\n",
        "# Scale data so that features have zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) # Use the same scaler fitted on training data\n",
        "print(\"Features scaled using StandardScaler.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 4. Create and Train the Logistic Regression Model\n",
        "print(\"\\n--- Training the Logistic Regression Model ---\")\n",
        "# 'max_iter' might need adjustment depending on the dataset for convergence\n",
        "# Using default solver 'lbfgs' which works well for many cases.\n",
        "log_reg_classifier = LogisticRegression(random_state=42, max_iter=200)\n",
        "print(f\"Using LogisticRegression(random_state=42, max_iter=200)\")\n",
        "\n",
        "# Train the model\n",
        "log_reg_classifier.fit(X_train_scaled, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 5. Make Predictions on the Test Set\n",
        "print(\"\\n--- Making Predictions on the Test Set ---\")\n",
        "y_pred_log = log_reg_classifier.predict(X_test_scaled)\n",
        "print(\"Predictions made on the scaled test set.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 6. Evaluate the Model\n",
        "print(\"\\n--- Evaluating the Model ---\")\n",
        "# Calculate Accuracy\n",
        "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy_log:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm_log = confusion_matrix(y_test, y_pred_log)\n",
        "print(cm_log)\n",
        "# Rows: Actual Classes (0: Malignant, 1: Benign)\n",
        "# Columns: Predicted Classes (0: Malignant, 1: Benign)\n",
        "# [[TN, FP],\n",
        "#  [FN, TP]]\n",
        "# TN = True Negatives (Actual Malignant, Predicted Malignant)\n",
        "# FP = False Positives (Actual Malignant, Predicted Benign)\n",
        "# FN = False Negatives (Actual Benign, Predicted Malignant)\n",
        "# TP = True Positives (Actual Benign, Predicted Benign)\n",
        "\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "# Use target_names from the *cancer* dataset\n",
        "report_log = classification_report(y_test, y_pred_log, target_names=cancer.target_names)\n",
        "print(report_log)\n",
        "# Precision: TP / (TP + FP)\n",
        "# Recall (Sensitivity): TP / (TP + FN)\n",
        "# F1-Score: Harmonic mean of Precision and Recall\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Evaluation Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP-WRDy166ZR",
        "outputId": "2099a313-18ad-4fd4-80d9-06946782d26b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "Dataset Description (Partial):\n",
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 569\n",
            "\n",
            ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            ":Attribute Information:\n",
            "    - radius (mean of distances from center to points on the perimeter)\n",
            "    - texture (standard deviation of gray-scale values)\n",
            "    - perimeter\n",
            "    - area\n",
            "    - smoothness (local variation in radius lengths)\n",
            "    - compactness (...\n",
            "\n",
            "Feature Names:\n",
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "\n",
            "Target Names:\n",
            "['malignant' 'benign']\n",
            "\n",
            "Shape of Features (X): (569, 30)\n",
            "Shape of Target (y): (569,)\n",
            "------------------------------\n",
            "\n",
            "--- Splitting Data ---\n",
            "Shape of X_train: (398, 30)\n",
            "Shape of y_train: (398,)\n",
            "Shape of X_test: (171, 30)\n",
            "Shape of y_test: (171,)\n",
            "------------------------------\n",
            "\n",
            "--- Scaling Features ---\n",
            "Features scaled using StandardScaler.\n",
            "------------------------------\n",
            "\n",
            "--- Training the Logistic Regression Model ---\n",
            "Using LogisticRegression(random_state=42, max_iter=200)\n",
            "Model training complete.\n",
            "------------------------------\n",
            "\n",
            "--- Making Predictions on the Test Set ---\n",
            "Predictions made on the scaled test set.\n",
            "------------------------------\n",
            "\n",
            "--- Evaluating the Model ---\n",
            "Logistic Regression Model Accuracy: 0.9883\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 63   1]\n",
            " [  1 106]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.98      0.98        64\n",
            "      benign       0.99      0.99      0.99       107\n",
            "\n",
            "    accuracy                           0.99       171\n",
            "   macro avg       0.99      0.99      0.99       171\n",
            "weighted avg       0.99      0.99      0.99       171\n",
            "\n",
            "------------------------------\n",
            "Evaluation Complete.\n"
          ]
        }
      ]
    }
  ]
}